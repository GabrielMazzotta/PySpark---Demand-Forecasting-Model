{"cells":[{"cell_type":"code","execution_count":1,"id":"d7c8e3c4-1103-4f10-bd02-bc1083a56cc5","metadata":{"executionCancelledAt":null,"executionTime":19834,"lastExecutedAt":1725288537173,"lastExecutedByKernel":"0d66c156-19fc-42ed-948f-2967b5289003","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import required libraries\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.regression import RandomForestRegressor\nfrom pyspark.sql.functions import col, dayofmonth, month, year,  to_date, to_timestamp, weekofyear, dayofweek\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\n# Initialize Spark session\nmy_spark = SparkSession.builder.appName(\"SalesForecast\").getOrCreate()\n\n# Importing sales data\nsales_data = my_spark.read.csv(\n    \"Online Retail.csv\", header=True, inferSchema=True, sep=\",\")\n\n# Convert InvoiceDate to datetime \nsales_data = sales_data.withColumn(\"InvoiceDate\", to_date(\n    to_timestamp(col(\"InvoiceDate\"), \"d/M/yyyy H:mm\")))\n\n# Aggregate data into daily intervals\ndaily_sales_data = sales_data.groupBy(\"Country\", \"StockCode\", \"InvoiceDate\", \"Year\", \"Month\", \"Day\", \"Week\", \"DayOfWeek\").agg({\"Quantity\": \"sum\",                                                                                                           \"UnitPrice\": \"avg\"})\n# Rename the target column\ndaily_sales_data = daily_sales_data.withColumnRenamed(\n    \"sum(Quantity)\", \"Quantity\")\n\n# Split the data into two sets based on the spliting date, \"2011-09-25\". All data up to and including this date should be in the training set, while data after this date should be in the testing set. Return a pandas Dataframe, pd_daily_train_data, containing, at least, the columns [\"Country\", \"StockCode\", \"InvoiceDate\", \"Quantity\"].\n\nsplit_date_train_test = \"2011-09-25\"\n\n# Creating the train and test datasets\ntrain_data = daily_sales_data.filter(\n    col(\"InvoiceDate\") <= split_date_train_test)\ntest_data = daily_sales_data.filter(col(\"InvoiceDate\") > split_date_train_test)\n\npd_daily_train_data = train_data.toPandas()\n\n# Creating indexer for categorical columns\ncountry_indexer = StringIndexer(\n    inputCol=\"Country\", outputCol=\"CountryIndex\").setHandleInvalid(\"keep\")\nstock_code_indexer = StringIndexer(\n    inputCol=\"StockCode\", outputCol=\"StockCodeIndex\").setHandleInvalid(\"keep\")\n\n# Selectiong features columns\nfeature_cols = [\"CountryIndex\", \"StockCodeIndex\", \"Month\", \"Year\",\n                \"DayOfWeek\", \"Day\", \"Week\"]\n\n# Using vector assembler to combine features\nassembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n\n# Initializing a Random Forest model\nrf = RandomForestRegressor(\n    featuresCol=\"features\",\n    labelCol=\"Quantity\",\n    maxBins=4000\n)\n\n# Create a pipeline for staging the processes\npipeline = Pipeline(stages=[country_indexer, stock_code_indexer, assembler, rf])\n\n# Training the model\nmodel = pipeline.fit(train_data)\n\n# Getting test predictions\ntest_predictions = model.transform(test_data)\ntest_predictions = test_predictions.withColumn(\n    \"prediction\", col(\"prediction\").cast(\"double\"))\n\n# Provide the Mean Absolute Error (MAE) for your forecast? Return a double/floar \"mae\"\n\n# Initializing the evaluator\nmae_evaluator = RegressionEvaluator(\n    labelCol=\"Quantity\", predictionCol=\"prediction\", metricName=\"mae\")\n\n# Obtaining MAE\nmae = mae_evaluator.evaluate(test_predictions)\n\n# How many units will be sold during the  week 39 of 2011? Return an integer `quantity_sold_w39`.\n\n# Getting the weekly sales of all countries\nweekly_test_predictions = test_predictions.groupBy(\"Year\", \"Week\").agg({\"prediction\": \"sum\"})\n\n# Finding the quantity sold on the 39 week. \npromotion_week = weekly_test_predictions.filter(col('Week')==39)\n\n# Storing prediction as quantity_sold_w30\nquantity_sold_w39 = int(promotion_week.select(\"sum(prediction)\").collect()[0][0])\n\n# Stop the Spark session\nmy_spark.stop()","outputsMetadata":{"0":{"height":38,"type":"stream"},"1":{"height":38,"type":"stream"},"2":{"height":38,"type":"stream"},"3":{"height":38,"type":"stream"},"4":{"height":38,"type":"stream"},"5":{"height":38,"type":"stream"},"6":{"height":38,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+----+--------------+---------+---------------+\n","|Year|Week|       Country|StockCode|sum(prediction)|\n","+----+----+--------------+---------+---------------+\n","|2011|  39|United Kingdom|    22838|             13|\n","|2011|  39|United Kingdom|    22674|             19|\n","|2011|  44|United Kingdom|    22436|             52|\n","|2011|  48|United Kingdom|    22227|             55|\n","|2011|  44|United Kingdom|    20972|            105|\n","|2011|  39|         Spain|    21879|             14|\n","|2011|  39|United Kingdom|    22695|             42|\n","|2011|  44|United Kingdom|    21070|              7|\n","|2011|  40|United Kingdom|   85194L|             17|\n","|2011|  40|United Kingdom|    22384|            139|\n","|2011|  44|United Kingdom|    84949|             30|\n","|2011|  44|United Kingdom|    22712|             46|\n","|2011|  40|United Kingdom|    22357|              7|\n","|2011|  44|United Kingdom|    21787|             45|\n","|2011|  48|United Kingdom|    21928|             67|\n","|2011|  40|United Kingdom|    22985|             25|\n","|2011|  49|       Germany|   84800L|             17|\n","|2011|  49|United Kingdom|    22770|              7|\n","|2011|  40|United Kingdom|    21265|             14|\n","|2011|  49|United Kingdom|    84987|             31|\n","+----+----+--------------+---------+---------------+\n","only showing top 20 rows\n","\n"]}],"source":["# Importing required libraries\n","from pyspark.sql import SparkSession\n","from pyspark.ml.feature import VectorAssembler\n","from pyspark.ml import Pipeline\n","from pyspark.ml.regression import RandomForestRegressor\n","from pyspark.sql.functions import col, dayofmonth, month, year,  to_date, to_timestamp, weekofyear, dayofweek\n","from pyspark.ml.feature import StringIndexer\n","from pyspark.ml.evaluation import RegressionEvaluator\n","\n","# Initializing Spark session\n","my_spark = SparkSession.builder.appName(\"SalesForecast\").getOrCreate()\n","\n","# Importing sales data\n","sales_data = my_spark.read.csv(\n","    \"Online Retail.csv\", header=True, inferSchema=True, sep=\",\")\n","\n","# Converting InvoiceDate to datetime \n","sales_data = sales_data.withColumn(\"InvoiceDate\", to_date(\n","    to_timestamp(col(\"InvoiceDate\"), \"d/M/yyyy H:mm\")))\n","\n","# Aggregating data into daily intervals\n","daily_sales_data = sales_data.groupBy(\"Country\", \"StockCode\", \"InvoiceDate\", \"Year\", \"Month\", \"Day\", \"Week\", \"DayOfWeek\").agg({\"Quantity\": \"sum\",                                                                                                           \"UnitPrice\": \"avg\"})\n","# Renaming the target column\n","daily_sales_data = daily_sales_data.withColumnRenamed(\n","    \"sum(Quantity)\", \"Quantity\")\n","\n","# Data Split based on the spliting date, \"2011-09-25\". \n","\n","split_date_train_test = \"2011-09-25\"\n","\n","# Creating the train and test datasets\n","train_data = daily_sales_data.filter(\n","    col(\"InvoiceDate\") <= split_date_train_test)\n","test_data = daily_sales_data.filter(col(\"InvoiceDate\") > split_date_train_test)\n","\n","pd_daily_train_data = train_data.toPandas()\n","\n","# Creating indexer for categorical columns\n","country_indexer = StringIndexer(\n","    inputCol=\"Country\", outputCol=\"CountryIndex\").setHandleInvalid(\"keep\")\n","stock_code_indexer = StringIndexer(\n","    inputCol=\"StockCode\", outputCol=\"StockCodeIndex\").setHandleInvalid(\"keep\")\n","\n","# Selecting features columns\n","feature_cols = [\"CountryIndex\", \"StockCodeIndex\", \"Month\", \"Year\",\n","                \"DayOfWeek\", \"Day\", \"Week\"]\n","\n","# Using vector assembler to combine features\n","assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n","\n","# Initializing a Random Forest model\n","rf = RandomForestRegressor(\n","    featuresCol=\"features\",\n","    labelCol=\"Quantity\",\n","    maxBins=4000\n",")\n","\n","# Creating a pipeline for staging the processes\n","pipeline = Pipeline(stages=[country_indexer, stock_code_indexer, assembler, rf])\n","\n","# Training the model\n","model = pipeline.fit(train_data)\n","\n","# Getting test predictions\n","test_predictions = model.transform(test_data)\n","test_predictions = test_predictions.withColumn(\n","    \"prediction\", col(\"prediction\").cast(\"double\"))\n","\n","# Providing Mean Absolute Error (MAE) for forecast\n","\n","# Initializing the evaluator\n","mae_evaluator = RegressionEvaluator(\n","    labelCol=\"Quantity\", predictionCol=\"prediction\", metricName=\"mae\")\n","\n","# Obtaining MAE\n","mae = mae_evaluator.evaluate(test_predictions)\n","\n","# Predicted units to be sold during the week 39 of 2011, per Country and product.\n","\n","# Getting the weekly sales\n","weekly_test_predictions = test_predictions.groupBy(\"Year\", \"Week\", \"Country\", \"StockCode\").agg({\"prediction\": \"sum\"})\n","weekly_test_predictions = weekly_test_predictions.withColumn(\"sum(prediction)\", col(\"sum(prediction)\").cast(\"integer\"))\n","\n","# Filtering for 39 week. \n","promotion_week = weekly_test_predictions.filter(col('Week')==39)\n","weekly_test_predictions.show()\n","\n","# Stop the Spark session\n","my_spark.stop()"]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":5}
